{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/143211/TARUN_INFO5731/blob/main/Konda_Tarun_Exercise_2_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does physical exercise affect resting heart rate and cardiovascular health in various age groups?\n",
        "\n",
        "This topic seeks to examine the association between physical activity and cardiovascular health, split by age, in order to find trends or patterns that may guide public health recommendations or individual health decisions.\n",
        "\n",
        "\n",
        "Data needed:\n",
        "To respond to this question, the dataset should contain:\n",
        "\n",
        "Age: To divide the data into several age groups.\n",
        "Weekly Physical exercise Hours: To determine the amount of physical exercise.\n",
        "Resting heart rate (RHR) is a measure of cardiovascular efficiency and health.\n",
        "Cardiovascular Health Status: A categorical variable based on RHR and physical activity levels that indicates overall cardiovascular health (Excellent, Good, Average, Poor).\n",
        "\n",
        "To conduct relevant analysis across several age groups and achieve statistical significance, a dataset of at least 1000 samples is recommended. More samples would allow for more precise age group segmentation and stronger findings.\n",
        "\n",
        "To gather and retain the data required for analysing the impact of physical exercise on cardiovascular health across age groups, we first generate synthetic data with a Python script. This script generates data points for 1000 people at random, including age, weekly physical activity hours, resting heart rate (RHR), and cardiovascular health status, resulting in a varied and complete dataset. Once created, the data is saved to a CSV file called 'cardiovascular_health_data.csv' using the pandas library's 'to_csv' function. This technique provides easy access and manipulation for future investigation. To ensure the dataset's integrity, data types and formats must be consistent. For real-world data gathering, this stage would be preceded by the creation of a complete data collecting strategy that included participant recruiting and ethical issues.\n"
      ],
      "metadata": {
        "id": "eZQ0scbVXwuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize empty lists for data collection\n",
        "age_data = []\n",
        "activity_level_data = []\n",
        "rhr_data = []  # Resting Heart Rate (RHR)\n",
        "\n",
        "# Generate data for exactly 1000 individuals\n",
        "for _ in range(1000):  # Ensures loop runs 1000 times for 1000 samples\n",
        "    age = random.randint(18, 80)  # Age between 18 and 80\n",
        "    activity_level = random.uniform(0, 10)  # Weekly physical activity in hours (0 to 10 hours)\n",
        "    rhr = random.randint(60, 100)  # Resting heart rate in bpm (60 to 100)\n",
        "\n",
        "    # Determine cardiovascular health status based on RHR and physical activity\n",
        "    if rhr < 70 and activity_level >= 5:\n",
        "        health_status = \"Excellent\"\n",
        "    elif 70 <= rhr <= 80 and activity_level >= 3:\n",
        "        health_status = \"Good\"\n",
        "    elif 80 < rhr <= 90 and activity_level < 3:\n",
        "        health_status = \"Average\"\n",
        "    else:\n",
        "        health_status = \"Poor\"\n",
        "\n",
        "    # Append the generated data to the lists\n",
        "    age_data.append(age)\n",
        "    activity_level_data.append(activity_level)\n",
        "    rhr_data.append(rhr)\n",
        "\n",
        "# Create a DataFrame from the collected data\n",
        "data = pd.DataFrame({\n",
        "    'Age': age_data,\n",
        "    'Weekly Physical Activity (hours)': activity_level_data,\n",
        "    'Resting Heart Rate (bpm)': rhr_data,\n",
        "    'Cardiovascular Health Status': [health_status for _ in range(1000)]\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file named 'cardiovascular_health_data.csv'\n",
        "data.to_csv('cardiovascular_health_data.csv', index=False)\n",
        "\n",
        "print(\"1000 samples collected and saved to cardiovascular_health_data.csv successfully.\")\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef535786-a01b-4ffe-a394-493dbb3abe94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 samples collected and saved to cardiovascular_health_data.csv successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "def retrieve_scholar_articles(search_term, year_start, year_end, total_articles):\n",
        "    scholar_url = \"https://scholar.google.com/scholar\"\n",
        "    fetched_articles = []  # To hold article data\n",
        "\n",
        "    # Iterate through pages, considering Google Scholar shows 10 articles per page\n",
        "    for offset in range(0, total_articles, 10):\n",
        "        query_params = {\n",
        "            \"q\": search_term,\n",
        "            \"as_ylo\": year_start,\n",
        "            \"as_yhi\": year_end,\n",
        "            \"start\": offset\n",
        "        }\n",
        "\n",
        "        # Simulate browser request\n",
        "        request_headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0\"\n",
        "        }\n",
        "\n",
        "        # Execute GET request with provided parameters and headers\n",
        "        request_response = requests.get(scholar_url, params=query_params, headers=request_headers)\n",
        "\n",
        "        if request_response.status_code == 200:\n",
        "            parsed_html = BeautifulSoup(request_response.text, 'html.parser')\n",
        "            search_results = parsed_html.find_all('div', class_='gs_ri')\n",
        "\n",
        "            for result in search_results:\n",
        "                extracted_article = {}\n",
        "\n",
        "                article_title = result.find('h3', class_='gs_rt')\n",
        "                if article_title:\n",
        "                    extracted_article['title'] = article_title.get_text()\n",
        "\n",
        "                article_info = result.find('div', class_='gs_a')\n",
        "                if article_info:\n",
        "                    info_text = article_info.get_text()\n",
        "                    extracted_article['info'] = info_text\n",
        "                    # Extract year from the info text\n",
        "                    extracted_article['year'] = info_text.split('-')[-1].strip()\n",
        "                    # Assuming authors are listed first\n",
        "                    extracted_article['authors'] = info_text.split('-')[0].strip()\n",
        "\n",
        "                article_abstract = result.find('div', class_='gs_rs')\n",
        "                if article_abstract:\n",
        "                    extracted_article['abstract'] = article_abstract.get_text()\n",
        "\n",
        "                fetched_articles.append(extracted_article)\n",
        "\n",
        "                if len(fetched_articles) >= total_articles:\n",
        "                    break\n",
        "        if len(fetched_articles) >= total_articles:\n",
        "            break\n",
        "\n",
        "    return fetched_articles\n",
        "\n",
        "def main():\n",
        "    search_query = \"information retrieval\"\n",
        "    publication_start_year = 2015\n",
        "    publication_end_year = 2023\n",
        "    articles_needed = 1000\n",
        "    scholar_articles = retrieve_scholar_articles(search_query, publication_start_year, publication_end_year, articles_needed)\n",
        "\n",
        "    with open(\"scholar_articles.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(scholar_articles, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Saved {len(scholar_articles)} articles to 'scholar_articles.json'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj0iz6BAay-n",
        "outputId": "5e115b70-28f9-4a49-a80e-0cd3e561ff18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 0 articles to 'scholar_articles.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install asyncpraw\n",
        "!pip install --upgrade asyncpraw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En6IOiTUc72E",
        "outputId": "910b990c-cc80-47ad-e50e-d20bd24b1f20"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting asyncpraw\n",
            "  Downloading asyncpraw-7.7.1-py3-none-any.whl (196 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/196.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/196.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.7/196.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<1 (from asyncpraw)\n",
            "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (3.9.3)\n",
            "Collecting aiosqlite<=0.17.0 (from asyncpraw)\n",
            "  Downloading aiosqlite-0.17.0-py3-none-any.whl (15 kB)\n",
            "Collecting asyncprawcore<3,>=2.1 (from asyncpraw)\n",
            "  Downloading asyncprawcore-2.4.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.18.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (4.0.3)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.9.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.18->asyncpraw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2024.2.2)\n",
            "Installing collected packages: aiosqlite, aiofiles, asyncprawcore, asyncpraw\n",
            "Successfully installed aiofiles-0.8.0 aiosqlite-0.17.0 asyncpraw-7.7.1 asyncprawcore-2.4.0\n",
            "Requirement already satisfied: asyncpraw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: aiofiles<1 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.8.0)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (3.9.3)\n",
            "Requirement already satisfied: aiosqlite<=0.17.0 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.17.0)\n",
            "Requirement already satisfied: asyncprawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.18.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (4.0.3)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.9.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.18->asyncpraw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import asyncpraw\n",
        "import pandas as pd\n",
        "\n",
        "async def fetch_reddit_posts(client_id, client_secret, username, password, user_agent, subreddits, limit_per_subreddit):\n",
        "    reddit = asyncpraw.Reddit(client_id=client_id,\n",
        "                              client_secret=client_secret,\n",
        "                              username=username,\n",
        "                              password=password,\n",
        "                              user_agent=user_agent)\n",
        "\n",
        "    posts_data = {\n",
        "        'Post ID': [],\n",
        "        'Author': [],\n",
        "        'Title': [],\n",
        "        'Comments Count': [],\n",
        "        'Score': [],\n",
        "        'Upvote Ratio': [],\n",
        "        'Flair': [],\n",
        "    }\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        subreddit = await reddit.subreddit(subreddit_name)\n",
        "        async for post in subreddit.hot(limit=limit_per_subreddit):\n",
        "            posts_data['Post ID'].append(post.id)\n",
        "            posts_data['Author'].append(str(post.author))\n",
        "            posts_data['Title'].append(post.title)\n",
        "            posts_data['Comments Count'].append(post.num_comments)\n",
        "            posts_data['Score'].append(post.score)\n",
        "            posts_data['Upvote Ratio'].append(post.upvote_ratio)\n",
        "            posts_data['Flair'].append(post.link_flair_text)\n",
        "\n",
        "        print(f\"Completed scraping {subreddit_name}; total posts collected: {len(posts_data['Post ID'])}\")\n",
        "\n",
        "    await reddit.close()\n",
        "    return pd.DataFrame(posts_data)\n",
        "\n",
        "async def main():\n",
        "    client_id = \"YOUR_CLIENT_ID\"\n",
        "    client_secret = \"YOUR_CLIENT_SECRET\"\n",
        "    username = \"YOUR_USERNAME\"\n",
        "    password = \"YOUR_PASSWORD\"\n",
        "    user_agent = \"YOUR_USER_AGENT\"\n",
        "\n",
        "    subreddits_to_scrape = ['india', 'worldnews', 'announcements', 'funny', 'AskReddit',\n",
        "                            'gaming', 'pics', 'science', 'movies', 'todayilearned']\n",
        "    limit_per_subreddit = 100  # Adjust as needed for your use case\n",
        "\n",
        "    df = await fetch_reddit_posts(client_id, client_secret, username, password, user_agent, subreddits_to_scrape, limit_per_subreddit)\n",
        "    df.to_csv('async_reddit_data.csv', index=False)\n",
        "    print(\"Data collection complete and saved to 'async_reddit_data.csv'.\")\n",
        "\n",
        "# Manual event loop handling\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        if loop.is_running():\n",
        "            task = loop.create_task(main())\n",
        "        else:\n",
        "            loop.run_until_complete(main())\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error running async main: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working on web scraping and data collecting tasks has been a fantastic learning experience, greatly improving my grasp of how to programmatically access and extract data from internet sources. Key concepts like BeautifulSoup's HTML structure navigation and Selenium's dynamic content management have proven essential. Challenges occurred mostly from dealing with websites that extensively rely on JavaScript for dynamic content rendering, necessitating a more sophisticated approach using Selenium to emulate browser interactions. Overcoming these challenges by learning to emulate user behaviours programmatically was very satisfying.In my area, the capacity to collect and analyse data from a variety of internet sources offers up new research opportunities, allowing for the creation of big datasets that may provide deeper insights and support evidence-based conclusions, so improving the quality and scope of my work.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mRYITlJjeIGg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}